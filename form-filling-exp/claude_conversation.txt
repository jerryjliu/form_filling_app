i am ceo of llamaindex. llamacloud is our commercial offering, offerign building blocks for document processing (that you can stitch together and use for genAI/RAG use cases, or IDP use cases like doc extraction). these building blocks include parse/extract/split/classify, also a sheets API for excel sheets.

i want to try building an app that adds an "edit" endpoint, so that i can actually make edits on existing docs. i imagine there's two types of PDFs - one where the PDF has directly fillable form fields, and another where you just want to draw on the PDF.

the closest reference is reducto's edit API: https://docs.reducto.ai/editing/edit-overview

before you start coding, i'm mostly curious to see what the implementation sketch of the core algorithm would be. e.g. what the core python function would look like. are there libs that would let me detect fillable fields and also draw entries on top?

Show less
3:45 PM

Thinking about designing a PDF editing algorithm for fillable forms and freehand annotations.


Let me first look at the Reducto docs you referenced to understand their approach, then I'll sketch out the core algorithm.

favicon

Fetched
Edit - Reducto API
reducto.ai


Architected multi-phase PDF editing algorithm with library recommendations.






This is helpful context. Now I have a clear picture of what Reducto does. Let me sketch out the core algorithm for you.

Implementation Sketch
There are essentially two paths based on PDF type:

Path 1: PDFs with AcroForm Fields (Fillable Forms)
These have structured form widgets embedded. You can programmatically detect and fill them.

Path 2: PDFs without Form Fields (Visual Detection)
Need to use vision/OCR to find "fillable areas" (blank lines, boxes), then draw text at those coordinates.

Core Libraries
Library	Purpose
PyMuPDF (fitz)	Best all-rounder - detect form fields, fill them, AND draw arbitrary text/shapes
pypdf	Good for AcroForm manipulation, simpler API
pikepdf	Lower-level, good for preserving PDF structure
ReportLab	Create overlay PDFs that get merged on top
PyMuPDF is probably your best bet since it handles both paths well.

Core Algorithm Sketch
python
from dataclasses import dataclass
from enum import Enum
import fitz  # PyMuPDF

class FieldType(Enum):
    TEXT = "text"
    CHECKBOX = "checkbox"
    DROPDOWN = "dropdown"
    RADIO = "radio"

@dataclass
class DetectedField:
    field_id: str
    field_type: FieldType
    bbox: tuple[float, float, float, float]  # (x0, y0, x1, y1)
    page: int
    label_context: str  # nearby text for semantic understanding
    options: list[str] | None = None  # for dropdowns/radios
    native_field_name: str | None = None  # if AcroForm field exists

@dataclass
class FieldEdit:
    field_id: str
    value: str | bool


def edit_pdf(
    pdf_bytes: bytes,
    edit_instructions: str,  # natural language
    form_schema: list[DetectedField] | None = None  # optional pre-defined schema
) -> bytes:
    """
    Core edit endpoint implementation.
    
    Returns: modified PDF as bytes
    """
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    # Step 1: Detect fields (or use provided schema)
    if form_schema:
        fields = form_schema
    else:
        fields = detect_fields(doc)
    
    # Step 2: Map instructions to fields via LLM
    edits = map_instructions_to_fields(edit_instructions, fields)
    
    # Step 3: Apply edits
    for edit in edits:
        field = next(f for f in fields if f.field_id == edit.field_id)
        apply_edit(doc, field, edit.value)
    
    return doc.tobytes()


def detect_fields(doc: fitz.Document) -> list[DetectedField]:
    """
    Detect all fillable fields in the PDF.
    Uses AcroForm widgets if present, falls back to vision detection.
    """
    fields = []
    
    for page_num, page in enumerate(doc):
        # Path 1: Check for native AcroForm widgets
        widgets = list(page.widgets())
        
        if widgets:
            for widget in widgets:
                fields.append(DetectedField(
                    field_id=f"page{page_num}_{widget.field_name}",
                    field_type=_widget_type_to_field_type(widget.field_type),
                    bbox=tuple(widget.rect),
                    page=page_num,
                    label_context=_extract_nearby_text(page, widget.rect),
                    options=widget.choice_values if widget.field_type == fitz.PDF_WIDGET_TYPE_COMBOBOX else None,
                    native_field_name=widget.field_name
                ))
        else:
            # Path 2: Vision-based detection for non-form PDFs
            detected = _vision_detect_fillable_areas(page)
            fields.extend(detected)
    
    return fields


def _vision_detect_fillable_areas(page: fitz.Page) -> list[DetectedField]:
    """
    Use heuristics + optional vision model to find fillable regions.
    
    Approaches:
    1. Line detection: find horizontal lines (underscores for "fill in blank")
    2. Box detection: find rectangles that look like input boxes
    3. OCR + layout: find labels like "Name:" and detect empty space after
    4. Vision LLM: send page image to GPT-4V/Claude to identify fields
    """
    fields = []
    page_num = page.number
    
    # Heuristic 1: Find horizontal lines (common "fill in the blank" pattern)
    paths = page.get_drawings()
    for path in paths:
        if _is_horizontal_line(path):
            bbox = _path_to_bbox(path)
            nearby_text = _extract_nearby_text(page, bbox)
            fields.append(DetectedField(
                field_id=f"page{page_num}_line_{len(fields)}",
                field_type=FieldType.TEXT,
                bbox=bbox,
                page=page_num,
                label_context=nearby_text
            ))
    
    # Heuristic 2: Find empty rectangles (checkbox/text box patterns)
    for path in paths:
        if _is_empty_rectangle(path):
            bbox = _path_to_bbox(path)
            # Small squares are likely checkboxes
            width, height = bbox[2] - bbox[0], bbox[3] - bbox[1]
            is_checkbox = abs(width - height) < 5 and width < 20
            
            fields.append(DetectedField(
                field_id=f"page{page_num}_box_{len(fields)}",
                field_type=FieldType.CHECKBOX if is_checkbox else FieldType.TEXT,
                bbox=bbox,
                page=page_num,
                label_context=_extract_nearby_text(page, bbox)
            ))
    
    # Heuristic 3: Vision LLM fallback for complex forms
    # if not fields:
    #     fields = _call_vision_llm_for_field_detection(page)
    
    return fields


def apply_edit(doc: fitz.Document, field: DetectedField, value: str | bool):
    """
    Apply a single edit to the PDF.
    """
    page = doc[field.page]
    
    if field.native_field_name:
        # Path 1: Native AcroForm field - use widget API
        for widget in page.widgets():
            if widget.field_name == field.native_field_name:
                if field.field_type == FieldType.CHECKBOX:
                    widget.field_value = value
                elif field.field_type == FieldType.DROPDOWN:
                    widget.field_value = value
                else:
                    widget.field_value = str(value)
                widget.update()
                break
    else:
        # Path 2: No native field - draw text directly on page
        rect = fitz.Rect(field.bbox)
        
        if field.field_type == FieldType.CHECKBOX:
            if value:
                # Draw a checkmark or X
                page.draw_line(rect.tl, rect.br, color=(0, 0, 0), width=1.5)
                page.draw_line(rect.tr, rect.bl, color=(0, 0, 0), width=1.5)
        else:
            # Insert text - adjust position to sit on the line
            text_point = fitz.Point(rect.x0, rect.y1 - 2)
            page.insert_text(
                text_point,
                str(value),
                fontsize=10,
                fontname="helv",
                color=(0, 0, 0)
            )


def map_instructions_to_fields(
    instructions: str, 
    fields: list[DetectedField]
) -> list[FieldEdit]:
    """
    Use LLM to map natural language instructions to specific fields.
    
    This is where the "AI" part happens - understanding context.
    """
    # Build field descriptions for the LLM
    field_descriptions = []
    for f in fields:
        desc = f"- {f.field_id} ({f.field_type.value}): nearby text = '{f.label_context}'"
        if f.options:
            desc += f", options = {f.options}"
        field_descriptions.append(desc)
    
    prompt = f"""Given these form fields:
{chr(10).join(field_descriptions)}

And these instructions:
{instructions}

Return a JSON array of edits: [{{"field_id": "...", "value": "..."}}]
Only include fields that should be filled based on the instructions.
For checkboxes, use true/false."""

    # Call your LLM here
    # response = llm.complete(prompt)
    # return parse_edits(response)
    
    pass  # placeholder


# Helper functions
def _extract_nearby_text(page: fitz.Page, rect: tuple, radius: int = 50) -> str:
    """Extract text near a bounding box to understand field context."""
    search_rect = fitz.Rect(rect).irect
    search_rect.x0 -= radius
    search_rect.y0 -= radius
    search_rect.x1 += radius
    search_rect.y1 += radius
    return page.get_text("text", clip=search_rect).strip()

def _widget_type_to_field_type(widget_type: int) -> FieldType:
    mapping = {
        fitz.PDF_WIDGET_TYPE_TEXT: FieldType.TEXT,
        fitz.PDF_WIDGET_TYPE_CHECKBOX: FieldType.CHECKBOX,
        fitz.PDF_WIDGET_TYPE_COMBOBOX: FieldType.DROPDOWN,
        fitz.PDF_WIDGET_TYPE_RADIOBUTTON: FieldType.RADIO,
    }
    return mapping.get(widget_type, FieldType.TEXT)
Key Design Decisions
PyMuPDF is the workhorse - handles both AcroForm detection/filling AND arbitrary drawing. Single dependency for both paths.
Form schema caching - Reducto returns the detected schema so you can reuse it. This is smart because field detection is expensive (especially vision-based). For the same form template, you detect once and reuse.
The hard part is vision detection - For non-AcroForm PDFs, detecting "where should text go" is genuinely hard. Options:
Heuristics (lines, boxes) - fast but brittle
Vision LLM - send page image to Claude/GPT-4V and ask "where are the fillable fields?"
Hybrid - heuristics first, vision LLM as fallback
Text overflow - Reducto mentions appendix pages for long text. You'd need to detect when text exceeds the bbox width and either truncate or create overflow pages.
Want me to expand on any part of this? The vision detection piece is probably the most complex - I can sketch out what that LLM prompt would look like if you're interested.